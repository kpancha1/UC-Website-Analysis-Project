{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting textstat\n",
      "  Downloading textstat-0.7.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting pyphen (from textstat)\n",
      "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from textstat) (58.0.4)\n",
      "Downloading textstat-0.7.4-py3-none-any.whl (105 kB)\n",
      "Downloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
      "Successfully installed pyphen-0.17.2 textstat-0.7.4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import pandas as pd  # For data handling\n",
    "import textstat\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Ensure the VADER lexicon is downloaded\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Function to extract text chunks from the webpage, clean them, and compute readability and sentiment statistics\n",
    "def fetch_and_analyze_readability(url):\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses\n",
    "        page = response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the page {url}: {e}\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame\n",
    "\n",
    "    # Parse the HTML page\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "\n",
    "    # Remove unwanted elements like scripts, styles, and comments\n",
    "    for script in soup([\"script\", \"style\", \"noscript\"]):\n",
    "        script.extract()  # Remove these elements from the soup\n",
    "\n",
    "    # Remove HTML comments\n",
    "    for comment in soup.findAll(text=lambda text: isinstance(text, Comment)):\n",
    "        comment.extract()\n",
    "\n",
    "    # Extract text chunks from paragraphs and headings\n",
    "    text_chunks = []\n",
    "    for element in soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "        text = element.get_text(strip=True)\n",
    "        if text:  # Ensure it's not empty\n",
    "            text_chunks.append(text)\n",
    "\n",
    "    # Initialize sentiment analyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # List to store results\n",
    "    results = []\n",
    "\n",
    "    # For each text chunk, compute statistics and sentiment\n",
    "    for chunk in text_chunks:\n",
    "        stats = compute_statistics(chunk)\n",
    "        sentiment = sid.polarity_scores(chunk)\n",
    "        # Determine sentiment category\n",
    "        if sentiment['compound'] >= 0.05:\n",
    "            sentiment_category = 'Positive'\n",
    "        elif sentiment['compound'] <= -0.05:\n",
    "            sentiment_category = 'Negative'\n",
    "        else:\n",
    "            sentiment_category = 'Neutral'\n",
    "        # Combine all results into a dictionary\n",
    "        result = {'URL': url, 'Text Chunk': chunk}\n",
    "        result.update(stats)\n",
    "        result['Sentiment'] = sentiment['compound']\n",
    "        result['Sentiment Category'] = sentiment_category\n",
    "        results.append(result)\n",
    "\n",
    "    # Create DataFrame from results\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "# Function to compute readability scores using textstat\n",
    "def compute_statistics(text):\n",
    "    stats = {}\n",
    "    # Calculate various readability metrics\n",
    "    stats['Flesch Reading Ease'] = textstat.flesch_reading_ease(text)\n",
    "    stats['Flesch-Kincaid Grade Level'] = textstat.flesch_kincaid_grade(text)\n",
    "    stats['SMOG Index'] = textstat.smog_index(text)\n",
    "    stats['Gunning Fog Index'] = textstat.gunning_fog(text)\n",
    "    stats['Automated Readability Index'] = textstat.automated_readability_index(text)\n",
    "    stats['Coleman Liau Index'] = textstat.coleman_liau_index(text)\n",
    "    stats['Dale-Chall Readability Score'] = textstat.dale_chall_readability_score(text)\n",
    "    stats['Linsear Write Formula'] = textstat.linsear_write_formula(text)\n",
    "    stats['Difficult Words'] = textstat.difficult_words(text)\n",
    "    stats['Total Number of Sentences'] = textstat.sentence_count(text)\n",
    "    stats['Total Number of Words'] = textstat.lexicon_count(text)\n",
    "    return stats\n",
    "\n",
    "# Function to read URLs from a newline-separated file and analyze each one\n",
    "def analyze_urls_from_file(filename):\n",
    "    # Read the CSV file and extract URLs from the \"Websites\" column\n",
    "    try:\n",
    "        df_urls = pd.read_csv(f\"data/{filename}\")\n",
    "        urls = df_urls['URL'].dropna().tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the CSV file: {e}\")\n",
    "        return\n",
    "\n",
    "    # Initialize an empty list to collect DataFrames\n",
    "    all_results = []\n",
    "\n",
    "    # Iterate over each URL and analyze readability\n",
    "    for index, url in enumerate(urls):\n",
    "        print(f\"Processing URL {index + 1}: {url}\")\n",
    "        df = fetch_and_analyze_readability(url)\n",
    "        if not df.empty:\n",
    "            all_results.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    if all_results:\n",
    "        final_df = pd.concat(all_results, ignore_index=True)\n",
    "        # Output the DataFrame into a markdown file\n",
    "        final_df.to_markdown('readability_analysis.md', index=False)\n",
    "        print(\"Analysis complete. Results saved to 'readability_analysis.md'.\")\n",
    "    else:\n",
    "        print(\"No results to display.\")\n",
    "\n",
    "# Example of running the analysis with a file\n",
    "filename = input(\"Please provide a CSV file containing URLs. This file should be within the data folder: \")  # Path to the file containing the URLs\n",
    "analyze_urls_from_file(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
