{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import pandas as pd  # For data handling\n",
    "import textstat\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Ensure the VADER lexicon is downloaded\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Function to extract text chunks from the webpage, clean them, and compute readability and sentiment statistics\n",
    "def fetch_and_analyze_readability(url):\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses\n",
    "        page = response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the page {url}: {e}\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame\n",
    "\n",
    "    # Parse the HTML page\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "\n",
    "    # Remove unwanted elements like scripts, styles, and comments\n",
    "    for script in soup([\"script\", \"style\", \"noscript\"]):\n",
    "        script.extract()  # Remove these elements from the soup\n",
    "\n",
    "    # Remove HTML comments\n",
    "    for comment in soup.findAll(text=lambda text: isinstance(text, Comment)):\n",
    "        comment.extract()\n",
    "\n",
    "    # Extract text chunks from paragraphs and headings\n",
    "    text_chunks = []\n",
    "    for element in soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "        text = element.get_text(strip=True)\n",
    "        if text:  # Ensure it's not empty\n",
    "            text_chunks.append(text)\n",
    "\n",
    "    # Initialize sentiment analyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # List to store results\n",
    "    results = []\n",
    "\n",
    "    # For each text chunk, compute statistics and sentiment\n",
    "    for chunk in text_chunks:\n",
    "        stats = compute_statistics(chunk)\n",
    "        sentiment = sid.polarity_scores(chunk)\n",
    "        # Determine sentiment category\n",
    "        if sentiment['compound'] >= 0.05:\n",
    "            sentiment_category = 'Positive'\n",
    "        elif sentiment['compound'] <= -0.05:\n",
    "            sentiment_category = 'Negative'\n",
    "        else:\n",
    "            sentiment_category = 'Neutral'\n",
    "        # Combine all results into a dictionary\n",
    "        result = {'URL': url, 'Text Chunk': chunk}\n",
    "        result.update(stats)\n",
    "        result['Sentiment'] = sentiment_category\n",
    "        results.append(result)\n",
    "\n",
    "    # Create DataFrame from results\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "# Function to compute readability scores using textstat\n",
    "def compute_statistics(text):\n",
    "    stats = {}\n",
    "    # Calculate various readability metrics\n",
    "    stats['Flesch Reading Ease'] = textstat.flesch_reading_ease(text)\n",
    "    stats['Flesch-Kincaid Grade Level'] = textstat.flesch_kincaid_grade(text)\n",
    "    stats['SMOG Index'] = textstat.smog_index(text)\n",
    "    stats['Gunning Fog Index'] = textstat.gunning_fog(text)\n",
    "    stats['Automated Readability Index'] = textstat.automated_readability_index(text)\n",
    "    stats['Coleman Liau Index'] = textstat.coleman_liau_index(text)\n",
    "    stats['Dale-Chall Readability Score'] = textstat.dale_chall_readability_score(text)\n",
    "    stats['Linsear Write Formula'] = textstat.linsear_write_formula(text)\n",
    "    stats['Difficult Words'] = textstat.difficult_words(text)\n",
    "    stats['Total Number of Sentences'] = textstat.sentence_count(text)\n",
    "    stats['Total Number of Words'] = textstat.lexicon_count(text)\n",
    "    return stats\n",
    "\n",
    "# Function to read URLs from a newline-separated file and analyze each one\n",
    "def analyze_urls_from_file(filename):\n",
    "    # Read the file and extract URLs\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            urls = [line.strip() for line in file if line.strip()]\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the file: {e}\")\n",
    "        return\n",
    "\n",
    "    # Initialize an empty list to collect DataFrames\n",
    "    all_results = []\n",
    "\n",
    "    # Iterate over each URL and analyze readability\n",
    "    for index, url in enumerate(urls):\n",
    "        print(f\"Processing URL {index + 1}: {url}\")\n",
    "        df = fetch_and_analyze_readability(url)\n",
    "        if not df.empty:\n",
    "            all_results.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    if all_results:\n",
    "        final_df = pd.concat(all_results, ignore_index=True)\n",
    "        # Output the DataFrame into a markdown file\n",
    "        final_df.to_markdown('readability_analysis.md', index=False)\n",
    "        print(\"Analysis complete. Results saved to 'readability_analysis.md'.\")\n",
    "    else:\n",
    "        print(\"No results to display.\")\n",
    "\n",
    "# Example of running the analysis with a file\n",
    "filename = input(\"Please provide a filename containing URLs: \")  # Path to the file containing the URLs\n",
    "analyze_urls_from_file(filename)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
